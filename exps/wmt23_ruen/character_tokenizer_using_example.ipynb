{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b4803c-2e9b-4b2d-8900-a569b060ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, sys; sys.path.append(  # noqa: E401, E702\n",
    "    (pathlib.Path(\".\").resolve().parent.parent / \".\").as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1d20a2-ad5e-41b2-8ec6-c14c3f2ca107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.types.tokenizers import CharacterTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4425cb37-d72c-4b0e-a585-6640a268caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97fdb456-d1e5-4b1c-9f35-61c072078b8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Class `CharacterTokenizer` may have only 1 instance!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unallowed_second_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCharacterTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_github/dl_dev/lib/types/commons.py:14\u001b[0m, in \u001b[0;36mBaseStrictSingleton.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` may have only 1 instance!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseStrictSingleton\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class `CharacterTokenizer` may have only 1 instance!"
     ]
    }
   ],
   "source": [
    "unallowed_second_tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abeb9ec1-f1b0-492e-b1f2-b13ac375d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.load(\n",
    "    pathlib.Path(\"/home/david_tyuman/my_github/dl_dev/character_tokenizer_dict.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4612312-1788-4338-833e-baea749cd146",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer's already fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msome_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_github/dl_dev/lib/types/tokenizers/base.py:36\u001b[0m, in \u001b[0;36mBaseTokenizer.fit\u001b[0;34m(self, path, limit)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: pathlib\u001b[38;5;241m.\u001b[39mPosixPath, limit: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_available_to_be_fitted:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms already fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl(path, limit)\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer's already fitted"
     ]
    }
   ],
   "source": [
    "tokenizer.fit(\"some_path\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68434803-8572-4fe4-94ef-473d06d97a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1, 20, 20, 6, 138]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"hello!\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ab2631-93eb-41d3-a8af-d8cf860aed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c02592-9b60-4511-a60e-4b3868b6e6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"asds1201n)ыфвфь!@)B(N))мг0тцс18И(Н.,;И1234567890-+_)(*&^%$#@1234567890_)(;.,:%пиро  N0710212js   'ad\"\n",
    "s == tokenizer.decode(tokenizer.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5004365c-7990-4e9d-9a78-c8d649688ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[157, 157, 157]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"���\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90535c6-0cc9-4095-b7e8-a65a6c776c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = tokenizer.decode([987654321123456789])\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd772d77-939c-43b6-9c66-a8243b5f9ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[157]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f2ad20-c04c-4cd6-a054-d35d98f3c42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, -1, -1, 106]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"A△▽W\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63315b6-0285-414a-817d-bcf2ff841b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 means unknown token, a token that hasn't been observed while tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3869eb0d-7b63-4e6f-a989-4dfe226c1acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A��W'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75895b2d-d3e9-4655-88d9-878ef987fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 35, 17, 14]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"⊲⊳ ⏏ ⌫ ⌧ ⌦ ⍓ ⍌ ⍃ывs\"\n",
    "tokens = tokenizer.encode(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc48b6d2-d030-4ff2-887d-782e55cb6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (tokenizer.decode(tokenizer.encode(s)) == s) is False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
